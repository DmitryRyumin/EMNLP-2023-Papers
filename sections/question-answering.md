# EMNLP-2023-Papers

<div align="center">
    <a href="https://github.com/DmitryRyumin/EMNLP-2023-Papers/blob/main/sections/human-centered-nlp.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/EMNLP-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/EMNLP-2023-Papers/blob/main/sections/resources-and-evaluation.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Question Answering

![Section Papers](https://img.shields.io/badge/Section%20Papers-5-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-soon-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-soon-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-0-FF0000)

<!-- 304 -->
| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [Compressing and Debiasing Vision-Language Pre-Trained Models for Visual Question Answering](https://aclanthology.org/2023.emnlp-main.34) | [![GitHub](https://img.shields.io/github/stars/PhoebusSi/Compress-Robust-VQA)](https://github.com/PhoebusSi/Compress-Robust-VQA) | [![acl](https://img.shields.io/badge/pdf-ACL%20Anthology-CBCBCC.svg)](https://aclanthology.org/2023.emnlp-main.34.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.14558-b31b1b.svg)](http://arxiv.org/abs/2210.14558) | :heavy_minus_sign: |
| [Merging Generated and Retrieved Knowledge for Open-Domain QA](https://aclanthology.org/2023.emnlp-main.286) | [![GitHub](https://img.shields.io/github/stars/yunx-z/COMBO)](https://github.com/yunx-z/COMBO) | [![acl](https://img.shields.io/badge/pdf-ACL%20Anthology-CBCBCC.svg)](https://aclanthology.org/2023.emnlp-main.286.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.14393-b31b1b.svg)](http://arxiv.org/abs/2310.14393) | :heavy_minus_sign: |
| [Diversity Enhanced Narrative Question Generation for Storybooks](https://aclanthology.org/2023.emnlp-main.31) | [![GitHub](https://img.shields.io/github/stars/hkyoon95/mQG)](https://github.com/hkyoon95/mQG) | [![acl](https://img.shields.io/badge/pdf-ACL%20Anthology-CBCBCC.svg)](https://aclanthology.org/2023.emnlp-main.31.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.16446-b31b1b.svg)](http://arxiv.org/abs/2310.16446) | :heavy_minus_sign: |
| [The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models](https://aclanthology.org/2023.emnlp-main.255) | [![GitHub](https://img.shields.io/github/stars/VT-NLP/SOCRATIC-QUESTIONING)](https://github.com/VT-NLP/SOCRATIC-QUESTIONING) | [![acl](https://img.shields.io/badge/pdf-ACL%20Anthology-CBCBCC.svg)](https://aclanthology.org/2023.emnlp-main.255.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.14999-b31b1b.svg)](http://arxiv.org/abs/2305.14999) | :heavy_minus_sign: |
| [Once Upon a Time in Graph: Relative-Time Pretraining for Complex Temporal Reasoning](https://aclanthology.org/2023.emnlp-main.728) | [![GitHub](https://img.shields.io/github/stars/DAMO-NLP-SG/RemeMo)](https://github.com/DAMO-NLP-SG/RemeMo) | [![acl](https://img.shields.io/badge/pdf-ACL%20Anthology-CBCBCC.svg)](https://aclanthology.org/2023.emnlp-main.728.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.14709-b31b1b.svg)](http://arxiv.org/abs/2310.14709) | :heavy_minus_sign: |
<!-- | On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method | Not found | -->
